{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnGHatCI51JP"
      },
      "source": [
        "# micrograd exercises\n",
        "\n",
        "1. watch the [micrograd video](https://www.youtube.com/watch?v=VMj-3S1tku0) on YouTube\n",
        "2. come back and complete these exercises to level up :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFt6NKOz6iBZ"
      },
      "source": [
        "## section 1: derivatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Jx9fCXl5xHd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.336362190988558\n"
          ]
        }
      ],
      "source": [
        "# here is a mathematical expression that takes 3 inputs and produces one output\n",
        "from math import sin, cos\n",
        "\n",
        "def f(a, b, c):\n",
        "  return -a**3 + sin(3*b) - 1.0/c + b**2.5 - a**0.5\n",
        "\n",
        "print(f(2, 3, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qXaH59eL9zxf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK for dim 0: expected -12.353553390593273, yours returns -12.353553390593273\n",
            "OK for dim 1: expected 10.25699027111255, yours returns 10.25699027111255\n",
            "OK for dim 2: expected 0.0625, yours returns 0.0625\n"
          ]
        }
      ],
      "source": [
        "# write the function df that returns the analytical gradient of f\n",
        "# i.e. use your skills from calculus to take the derivative, then implement the formula\n",
        "# if you do not calculus then feel free to ask wolframalpha, e.g.:\n",
        "# https://www.wolframalpha.com/input?i=d%2Fda%28sin%283*a%29%29%29\n",
        "\n",
        "def gradf(a, b, c):\n",
        "  # f'(a) = -3*a**2 - 0.5*a**(-0.5)\n",
        "  #       = -3*a**2 - (1/(4 * sqrt(a)))\n",
        "  \n",
        "  # f'(b) = 3 * cos(3*b) + 2.5 * b ** 1.5\n",
        "  \n",
        "  # f'(c) = 1 / c**2\n",
        "  \n",
        "  dfda = -3*a**2 - 0.5*a**(-0.5)\n",
        "  dfdb = 3*cos(3*b) + 2.5*(b**1.5)\n",
        "  dfdc = 1 / (c**2)\n",
        "  return [dfda, dfdb, dfdc] # todo, return [df/da, df/db, df/dc]\n",
        "\n",
        "# expected answer is the list of\n",
        "ans = [-12.353553390593273, 10.25699027111255, 0.0625]\n",
        "yours = gradf(2, 3, 4)\n",
        "for dim in range(3):\n",
        "  ok = 'OK' if abs(yours[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {yours[dim]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_27n-KTA9Qla"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WRONG! for dim 0: expected -12.353553390593273, yours returns -12.353612948956536\n",
            "WRONG! for dim 1: expected 10.25699027111255, yours returns 10.257004202163245\n",
            "OK for dim 2: expected 0.0625, yours returns 0.06249984378925432\n"
          ]
        }
      ],
      "source": [
        "# now estimate the gradient numerically without any calculus, using\n",
        "# the approximation we used in the video.\n",
        "# you should not call the function df from the last cell\n",
        "\n",
        "h = 0.00001\n",
        "num_dfda = (f(2+h, 3, 4) - f(2, 3, 4)) / h\n",
        "num_dfdb = (f(2, 3+h, 4) - f(2, 3, 4)) / h\n",
        "num_dfdc = (f(2, 3, 4+h) - f(2, 3, 4)) / h\n",
        "\n",
        "numerical_grad = [num_dfda, num_dfdb, num_dfdc]\n",
        "\n",
        "\n",
        "for dim in range(3):\n",
        "  ok = 'OK' if abs(numerical_grad[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad[dim]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BUqsGb5o_h2P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK for dim 0: expected -12.353553390593273, yours returns -12.353553390820336\n",
            "OK for dim 1: expected 10.25699027111255, yours returns 10.256990271617639\n",
            "OK for dim 2: expected 0.0625, yours returns 0.06250000001983835\n"
          ]
        }
      ],
      "source": [
        "# there is an alternative formula that provides a much better numerical\n",
        "# approximation to the derivative of a function.\n",
        "# learn about it here: https://en.wikipedia.org/wiki/Symmetric_derivative\n",
        "# implement it. confirm that for the same step size h this version gives a\n",
        "# better approximation.\n",
        "\n",
        "# -----------\n",
        "h = 0.00001\n",
        "num_dfda_2 = (f(2+h, 3, 4) - f(2-h, 3, 4)) / (2*h)\n",
        "num_dfdb_2 = (f(2, 3+h, 4) - f(2, 3-h, 4)) / (2*h)\n",
        "num_dfdc_2 = (f(2, 3, 4+h) - f(2, 3, 4-h)) / (2*h)\n",
        "\n",
        "numerical_grad2 = [num_dfda_2, num_dfdb_2, num_dfdc_2]\n",
        "# -----------\n",
        "\n",
        "for dim in range(3):\n",
        "  ok = 'OK' if abs(numerical_grad2[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad2[dim]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tklF9s_4AtlI"
      },
      "source": [
        "## section 2: support for softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAPe_RVrCTeO"
      },
      "outputs": [],
      "source": [
        "# Value class starter code, with many functions taken out\n",
        "from math import exp, log\n",
        "\n",
        "class Value:\n",
        "\n",
        "  def __init__(self, data, _children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0.0\n",
        "    self._backward = lambda: None\n",
        "    self._prev = set(_children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data}, op={self._op}, label={self.label})\"\n",
        "\n",
        "  def __add__(self, other): # exactly as in the video\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += 1.0 * out.grad\n",
        "      other.grad += 1.0 * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "  \n",
        "  def __radd__(self, other):\n",
        "    return self + other\n",
        "  \n",
        "  def __sub__(self, other):\n",
        "    return self + (-other)\n",
        "  \n",
        "  def __rsub__(self, other):\n",
        "    return self + (-other)\n",
        "  \n",
        "  def __mul__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data * other.data, (self, other), \"*\")\n",
        "    \n",
        "    def _backward():\n",
        "      self.grad += out.grad * other.data\n",
        "      other.grad += out.grad * self.data\n",
        "    out._backward = _backward\n",
        "    \n",
        "    return out\n",
        "  \n",
        "  def __rmul__(self, other):\n",
        "    return self * other\n",
        "  \n",
        "  def __pow__(self, other):\n",
        "    assert isinstance(other, (int, float))\n",
        "    out = Value(self.data ** other, (self, Value(other)), \"**\")\n",
        "    \n",
        "    def _backward():\n",
        "      self.grad += (other * self.data ** (other-1)) * out.grad \n",
        "    out._backward = _backward\n",
        "    \n",
        "    return out\n",
        "  \n",
        "  def __truediv__(self, other):\n",
        "    return self * other ** -1\n",
        "  \n",
        "  def __rtruediv__(self, other):\n",
        "    return other * self ** -1\n",
        "  \n",
        "  def exp(self):\n",
        "    out = Value(exp(self.data), (self, ), \"exp\")\n",
        "    \n",
        "    def _backward():\n",
        "      self.grad += out.grad * out.data\n",
        "    out._backward = _backward\n",
        "    \n",
        "    return out\n",
        "  \n",
        "  def __neg__(self):\n",
        "    return -1 * self\n",
        "  \n",
        "  def log(self):\n",
        "    out = Value(log(self.data), (self, ), \"log\")\n",
        "    \n",
        "    def _backward():\n",
        "      self.grad += (1 / self.data) * out.grad\n",
        "    out._backward = _backward\n",
        "    \n",
        "    return out\n",
        "\n",
        "  def backward(self): # exactly as in video\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(v)\n",
        "    build_topo(self)\n",
        "\n",
        "    self.grad = 1.0\n",
        "    for node in reversed(topo):\n",
        "      node._backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "def trace(root):\n",
        "    nodes, edges = set(), set()\n",
        "    def build(v):\n",
        "        if v not in nodes:\n",
        "            nodes.add(v)\n",
        "            for child in v._prev:\n",
        "                edges.add((child, v))\n",
        "                build(child)\n",
        "    build(root)\n",
        "    return nodes, edges\n",
        "    \n",
        "def draw_dot(root):\n",
        "    dot = Digraph(format=\"svg\", graph_attr={\"rankdir\": \"LR\"})\n",
        "    \n",
        "    nodes, edges = trace(root)\n",
        "    for n in nodes:\n",
        "        uid = str(id(n))\n",
        "        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape = \"record\")\n",
        "        if n._op:\n",
        "            dot.node(name = uid + n._op, label = n._op)\n",
        "            dot.edge(uid + n._op, uid)\n",
        "    for n1, n2 in edges:\n",
        "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
        "    \n",
        "    return dot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "VgWvwVQNAvnI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.1755153626167147\n",
            "OK for dim 0: expected 0.041772570515350445, yours returns 0.041772570515350445\n",
            "OK for dim 1: expected 0.8390245074625319, yours returns 0.8390245074625319\n",
            "OK for dim 2: expected 0.005653302662216329, yours returns 0.005653302662216329\n",
            "OK for dim 3: expected -0.8864503806400986, yours returns -0.8864503806400986\n"
          ]
        }
      ],
      "source": [
        "# without referencing our code/video __too__ much, make this cell work\n",
        "# you'll have to implement (in some cases re-implemented) a number of functions\n",
        "# of the Value object, similar to what we've seen in the video.\n",
        "# instead of the squared error loss this implements the negative log likelihood\n",
        "# loss, which is very often used in classification.\n",
        "\n",
        "# this is the softmax function\n",
        "# https://en.wikipedia.org/wiki/Softmax_function\n",
        "def softmax(logits):\n",
        "  counts = [logit.exp() for logit in logits]\n",
        "  denominator = sum(counts)\n",
        "  out = [c / denominator for c in counts]\n",
        "  return out\n",
        "\n",
        "# this is the negative log likelihood loss function, pervasive in classification\n",
        "logits = [Value(0.0), Value(3.0), Value(-2.0), Value(1.0)]\n",
        "probs = softmax(logits) # normalize\n",
        "loss = -probs[3].log() # take log and take the label matching index (dim 3 acts as the label for this input example)\n",
        "loss.backward()\n",
        "print(loss.data)\n",
        "\n",
        "\n",
        "ans = [0.041772570515350445, 0.8390245074625319, 0.005653302662216329, -0.8864503806400986]\n",
        "for dim in range(4):\n",
        "  ok = 'OK' if abs(logits[dim].grad - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {logits[dim].grad}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NLL, CE, BCE etc..\n",
        "\n",
        "\n",
        "#### Negative Log Likelihood (in a binary classification setting)\n",
        "\n",
        "$$\n",
        "\\begin{equation*}\n",
        "\\begin{align}\n",
        "\\mathbb{P}(\\mathcal{D} \\mid \\theta) = \\prod_{i=1}^{n} \\hat{y}_{\\theta, i}^{y_i} (1 - \\hat{y}_{\\theta, i})^{1 - y_i}  \\\\\n",
        "\\log \\mathbb{P}(\\mathcal{D} \\mid \\theta) = \\sum_{i=1}^{n} \\left( y_i \\log \\hat{y}_{\\theta, i} + (1 - y_i) \\log (1 - \\hat{y}_{\\theta, i}) \\right)  \\\\\n",
        "l(\\theta) = -\\sum_{i=1}^{n} \\left( y_i \\log \\hat{y}_{\\theta, i} + (1 - y_i) \\log (1 - \\hat{y}_{\\theta, i}) \\right)  \n",
        "\\end{align}\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "* Start with predicted probabilities for the positive class (y_hat). If we were given raw prediction values, apply sigmoid to make it a probability.\n",
        "* Compute the probabilities for the negative class (1-y_hat).\n",
        "* Compute the log probabilities.\n",
        "* Summing up the log probabilities associated with the true labels.\n",
        "\n",
        "In summary, **normalize**, **take log**, **sum up the log probs of the true labels**.  \n",
        "\n",
        "Sine the logarithmic function is monotonic, maximizing the likelihood is the same as maximizing the log likelihood. However, since minimizing loss makes more sense, we take the negative of the log likelihood function.\n",
        "\n",
        "\n",
        "#### Negative Log Likelihood (in a multiclass setting)\n",
        "\n",
        "* softmax is used to normalize the probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "q7ca1SVAGG1S"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 0.0418,  0.8390,  0.0057, -0.8865], dtype=torch.float64)"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# verify the gradient using the torch library\n",
        "# torch should give you the exact same gradient\n",
        "import torch\n",
        "\n",
        "logits = torch.tensor([0.0, 3.0, -2.0, 1.0]).double() ; logits.requires_grad = True\n",
        "m = torch.nn.Softmax(dim=0)\n",
        "probs = m(logits)\n",
        "loss = -torch.log(probs[3])\n",
        "loss.backward()\n",
        "\n",
        "logits.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "zero2hero",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
